{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the uniprot ids of the HuRi data\n",
    "\n",
    "At the start of the project, a new HQ PPI dataset has been released (HuRi).\n",
    "\n",
    "- https://www.nature.com/articles/s41586-020-2188-x\n",
    "- http://www.interactome-atlas.org/download\n",
    "\n",
    "There is a tsv file with only gene name combinations while we want the UniProtKB ids.\n",
    "There is also a PSI-MI file where more informations is stored.\n",
    "\n",
    "There is the github page on how to interpret the PSI-MI file:\n",
    "\n",
    "- https://github.com/HUPO-PSI/miTab/blob/master/PSI-MITAB27Format.md\n",
    "\n",
    "This script tries to extract the UniProtIDs from the PSI-MI file\n",
    "\n",
    "It turns out that biomart can map more ensp ids to uniprot than uniprot so this is the first thats being done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import huri PSI-MI file\n",
    "df_huri = pd.read_csv('../Data/HuRi/HuRI_04_05_2020.psi', sep='\\t', header=None)\n",
    "# filter PPIs from the test screens\n",
    "df_huri_no_test = df_huri[~df_huri[27].str.contains('test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uniprotkb:Q13515</td>\n",
       "      <td>uniprotkb:Q9UJW9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uniprotkb:P30049</td>\n",
       "      <td>uniprotkb:Q05519-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ensembl:ENSP00000462298.1</td>\n",
       "      <td>uniprotkb:P43220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0                   1\n",
       "2           uniprotkb:Q13515    uniprotkb:Q9UJW9\n",
       "3           uniprotkb:P30049  uniprotkb:Q05519-2\n",
       "4  ensembl:ENSP00000462298.1    uniprotkb:P43220"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_huri_no_test.iloc[2:5, 0:2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some uniprot ids are still in the list because they only have a connection with one\n",
    "# We should check if we can map the ensembl ids to uniprotkb ids\n",
    "all_proteins = df_huri_no_test[[0,1]].values.reshape(-1)\n",
    "\n",
    "ensmble_ids = pd.Series([x.split(':')[1] for x in set(all_proteins) if x.split(':')[0] == 'ensembl'])\n",
    "# Write all ensemble ids to csv\n",
    "ensmble_ids.to_csv('../Data/HuRi/ensmble_ids_without_uniprot_id.csv', header=False, index=False)\n",
    "# BioMart can map some of them to UniProt\n",
    "df_mapping = pd.read_csv('../Data/IDMapping/huri_ensemble_ids_mapping_without_uniprot.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mapping_dict_swiss = pd.Series(df_mapping.iloc[:,1].values, index=df_mapping.iloc[:,0]).to_dict()\n",
    "mapping_dict_trembl = pd.Series(df_mapping.iloc[:,2].values, index=df_mapping.iloc[:,0]).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    x = x.split(':')[1]\n",
    "      \n",
    "    if x in ensmble_ids.to_list():\n",
    "        if x in mapping_dict_swiss.keys() and mapping_dict_swiss[x] == mapping_dict_swiss[x]:\n",
    "            x = mapping_dict_swiss[x]\n",
    "        if x in mapping_dict_swiss.keys() and mapping_dict_trembl[x] == mapping_dict_trembl[x]:\n",
    "            x = mapping_dict_trembl[x]\n",
    "    \n",
    "    return x\n",
    "\n",
    "conv_ids = pd.DataFrame({0:df_huri_no_test[0].apply(convert), 1:df_huri_no_test[1].apply(convert)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ENSP ids in the UniProtID column 58\n",
      "Number of ENSP ids in UniProt 24\n",
      "Number of ENSP ids in unable to map 24\n"
     ]
    }
   ],
   "source": [
    "conv_all_proteins = conv_ids.values.reshape(-1)\n",
    "conv_ensmble_ids = pd.Series([x for x in set(conv_all_proteins) if 'ENSP' in x])\n",
    "\n",
    "print('Number of ENSP ids in the UniProtID column', len(ensmble_ids))\n",
    "print('Number of ENSP ids in UniProt', len(conv_ensmble_ids))\n",
    "#add six because 6 proteins were not found at all using uniprot\n",
    "#they were not added to the conv dict\n",
    "print('Number of ENSP ids in unable to map', 6 + sum(df_mapping.iloc[:,1:3].isna().all(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENSP00000445323.1\n",
      "ENSP00000457957.1\n",
      "ENSP00000482225.1\n",
      "ENSP00000493046.1\n",
      "ENSP00000493281.1\n",
      "ENSP00000473509.2\n",
      "ENSP00000324909.5\n",
      "ENSP00000391869.3\n",
      "ENSP00000482011.1\n",
      "ENSP00000489519.1\n",
      "ENSP00000405973.1\n",
      "ENSP00000465194.1\n",
      "ENSP00000490118.1\n",
      "ENSP00000476265.1\n",
      "ENSP00000470609.1\n",
      "ENSP00000463483.1\n",
      "ENSP00000467185.1\n",
      "ENSP00000408168.2\n",
      "ENSP00000493302.1\n",
      "ENSP00000426769.1\n",
      "ENSP00000408321.1\n",
      "ENSP00000493122.1\n",
      "ENSP00000411822.1\n",
      "ENSP00000415200.2\n"
     ]
    }
   ],
   "source": [
    "# These ensp ids could not be converted\n",
    "# Lets check them manually by removing the version\n",
    "# sequence comparison\n",
    "for x in conv_ensmble_ids:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENSP00000445323\n",
      "ENSP00000457957\n",
      "ENSP00000482225\n",
      "ENSP00000493046\n",
      "ENSP00000493281\n",
      "ENSP00000473509\n",
      "ENSP00000324909\n",
      "ENSP00000391869\n",
      "ENSP00000482011\n",
      "ENSP00000489519\n",
      "ENSP00000405973\n",
      "ENSP00000465194\n",
      "ENSP00000490118\n",
      "ENSP00000476265\n",
      "ENSP00000470609\n",
      "ENSP00000463483\n",
      "ENSP00000467185\n",
      "ENSP00000408168\n",
      "ENSP00000493302\n",
      "ENSP00000426769\n",
      "ENSP00000408321\n",
      "ENSP00000493122\n",
      "ENSP00000411822\n",
      "ENSP00000415200\n"
     ]
    }
   ],
   "source": [
    "## remove version\n",
    "for x in conv_ensmble_ids:\n",
    "    print(x.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort to filter redundant combinations\n",
    "df_uniprot_huri = pd.DataFrame(df_huri[[0, 1]].apply(lambda x: sorted(x), axis=1).to_list())\n",
    "df_uniprot_huri_no_test = pd.DataFrame(df_huri_no_test[[0, 1]].apply(lambda x: sorted(x), axis=1).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter redundant combinations\n",
    "df_uniprot_comb = df_uniprot_huri.drop_duplicates()\n",
    "df_uniprot_comb_no_test = df_uniprot_huri_no_test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper PPI claim: 52569\n",
      "PSI-MI PPIs number without test results: 52237\n"
     ]
    }
   ],
   "source": [
    "# Now something weird is happening. \n",
    "# The numbers do not match with the papers claim. \n",
    "print('The paper PPI claim: 52569')\n",
    "# print('PSI-MI PPIs number:', len(df_uniprot_comb))\n",
    "print('PSI-MI PPIs number without test results:', len(df_uniprot_comb_no_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see if we can find the reason using the other tsv file HuRi provides on the website\n",
    "# This file only contains the ENSG id of interactor A and B\n",
    "\n",
    "# load the ENSG data\n",
    "df_huri_ensg =  pd.read_csv('../Data/HuRi/HuRI_genes_04_05_2020.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuRi gene combination number: 52548\n"
     ]
    }
   ],
   "source": [
    "# the numebr of combinations doesnt match the paper, but maybe there are redundant gene combinations\n",
    "# due to splice variation\n",
    "df_huri_ensg = pd.DataFrame(df_huri_ensg[[0, 1]].apply(lambda x: sorted(x), axis=1).to_list())\n",
    "print('HuRi gene combination number:', len(df_huri_ensg.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    uniprotkb:O75920-1\n",
       "1      uniprotkb:A1L3X0\n",
       "Name: 114, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is annoying since we now need the ENSG mapping of the Uniprot id\n",
    "# to figure out why we have less PPIs than the paper claims.\n",
    "# The ENSG ids of each interactor are in the PSI file but apparently\n",
    "# some of UniProt ids map to multiple ENSG ids.\n",
    "df_huri_no_test.iloc[113,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple ENSG ids column 2? True\n"
     ]
    }
   ],
   "source": [
    "# prove that there can multiple ENSGs in one cell\n",
    "# if the split list length is greater than 2, multiple ENSGs are found\n",
    "check_one_ensg_id = lambda x: len(x.split('ENSG')) > 2\n",
    "print('multiple ENSG ids column 2?', True in df_huri[2].apply(check_one_ensg_id).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with multiple ENSG ids: 903\n"
     ]
    }
   ],
   "source": [
    "#now see how many rows have multiple ENSGs in one of the two columns\n",
    "df_multiple_ensg = df_huri_no_test[df_huri_no_test[2].apply(check_one_ensg_id) | \n",
    "                                   df_huri_no_test[3].apply(check_one_ensg_id)]\n",
    "print('Number of entries with multiple ENSG ids:', len(df_multiple_ensg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple UniProt ids column 0? False\n",
      "multiple UniProt ids column 1? False\n"
     ]
    }
   ],
   "source": [
    "# They have max one uniprot mapping\n",
    "check_multiple_uniprot_ids = lambda x: len(x.split('uniprotkb')) > 2\n",
    "print('multiple UniProt ids column 0?', True in df_huri[0].apply(check_multiple_uniprot_ids).unique())\n",
    "print('multiple UniProt ids column 1?', True in df_huri[1].apply(check_multiple_uniprot_ids).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets just try to create all possible combinations of ENSG ids to see\n",
    "# if this number matches the tsv file. \n",
    "\n",
    "list_of_genes_A = df_huri_no_test[2].apply(\n",
    "                            lambda x: [i[i.find(':')+1:] for i in x.split('|')[2::3]]\n",
    "                ).to_list()\n",
    "\n",
    "list_of_genes_B = df_huri_no_test[3].apply(\n",
    "                            lambda x: [i[i.find(':')+1:] for i in x.split('|')[2::3]]\n",
    "                ).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows is the PSI file: 167924\n",
      "Possible ENSG id combinations with redundancy: 168956\n"
     ]
    }
   ],
   "source": [
    "#create a list with tuples of all combinations of ESNG ids and sort the tuples\n",
    "all_combinations = [list(itertools.product(a, b)) for a,b in zip(list_of_genes_A,list_of_genes_B)]\n",
    "all_combinations_list = [tuple(sorted(x)) for x in itertools.chain.from_iterable(all_combinations)]\n",
    "\n",
    "print('Total rows is the PSI file:', len(df_huri_no_test))\n",
    "print('Possible ENSG id combinations with redundancy:', len(all_combinations_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique ENSG id combinations from tsv: 52548\n",
      "Unique ENSG id combinations from PSI: 52548\n"
     ]
    }
   ],
   "source": [
    "#So there it is our number, it matches :D\n",
    "# Now we now where the ENSG combination number comes from.\n",
    "\n",
    "print('Unique ENSG id combinations from tsv:', len(df_huri_ensg.drop_duplicates()))\n",
    "print('Unique ENSG id combinations from PSI:', len(set(all_combinations_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every row in the HuRi PSI-MI file has one uniprot id for each interactor\n",
    "# The number of unique uniprot id combinations is lower than the paper claims so that is weird\n",
    "# Maybe they used all unique combination of ENSP to get to this number?\n",
    "\n",
    "list_of_proteins_A = df_huri_no_test[2].apply(\n",
    "                            lambda x: [i[i.find(':')+1:] for i in x.split('|')[1::3]]\n",
    "                ).to_list()\n",
    "\n",
    "list_of_proteins_B = df_huri_no_test[3].apply(\n",
    "                            lambda x: [i[i.find(':')+1:] for i in x.split('|')[1::3]]\n",
    "                ).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list with tuples of all combinations of ENSP ids and sort the tuples\n",
    "all_comb_prot = [list(itertools.product(a, b)) for a,b in zip(list_of_proteins_A,list_of_proteins_B)]\n",
    "all_comb_prot_list = [tuple(sorted(x)) for x in itertools.chain.from_iterable(all_comb_prot)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame with unique ENSP combinations\n",
    "ensp_comb_huri_final = pd.DataFrame(all_comb_prot_list).drop_duplicates()\n",
    "ensp_comb_huri_final.iloc[:,0] = ensp_comb_huri_final.iloc[:,0].apply(lambda x: x.split('.')[0])\n",
    "ensp_comb_huri_final.iloc[:,1] = ensp_comb_huri_final.iloc[:,1].apply(lambda x: x.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper PPIs claim: 52569\n",
      "Unique protein combinations using ESPN ids from PSI_MI: 52549\n"
     ]
    }
   ],
   "source": [
    "# Nope, it's not the number we are looking for\n",
    "# We are somehow missing 20 PPIs\n",
    "print('Paper PPIs claim: 52569')\n",
    "print('Unique protein combinations using ESPN ids from PSI_MI:', len(ensp_comb_huri_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter all combinations without a uniprot id\n",
    "uniprot_comb_huri_final = conv_ids[~conv_ids[[0,1]]\\\n",
    "                                .apply(lambda x: 'ENSP' in x[0] or 'ENSP' in x[1],  axis=1)]\n",
    "uniprot_comb_huri_final = pd.DataFrame(uniprot_comb_huri_final[[0, 1]].apply(lambda x: sorted(x), axis=1).to_list())\n",
    "uniprot_comb_huri_final = uniprot_comb_huri_final.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper PPIs claim: 52569\n",
      "Number of unique combinations in columns 0 and 1 of the psi file: 52236\n",
      "Unique protein combinations using UniProt ids from PSI_MI: 51767\n",
      "Unique protein combinations using ENSP ids from PSI_MI: 52549\n"
     ]
    }
   ],
   "source": [
    "#get all unique combinations after BioMart Mapping using column 0 and 1\n",
    "total_comb_map = df_huri_ensg = pd.DataFrame(conv_ids[[0, 1]].apply(lambda x: sorted(x), axis=1).to_list())\n",
    "\n",
    "print('Paper PPIs claim: 52569')\n",
    "print('Number of unique combinations in columns 0 and 1 of the psi file:', len(total_comb_map.drop_duplicates()))\n",
    "print('Unique protein combinations using UniProt ids from PSI_MI:', len(uniprot_comb_huri_final))\n",
    "print('Unique protein combinations using ENSP ids from PSI_MI:', len(ensp_comb_huri_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper claim of unique protein: 8275\n",
      "Number of unique proteins in columns 0 and 1 of the psi file: 8215\n",
      "Number of unique UniProt ids: 8184\n",
      "Number of unique ENSP ids: 8274\n"
     ]
    }
   ],
   "source": [
    "# ENSP and UniProt have the different numbers of unique ids\n",
    "all_unique_uniprot_ids = pd.Series(pd.concat([uniprot_comb_huri_final[0],uniprot_comb_huri_final[1]]).unique())\n",
    "all_unique_ensp_ids = pd.Series(pd.concat([ensp_comb_huri_final[0],ensp_comb_huri_final[1]]).unique())\n",
    "\n",
    "print('Paper claim of unique protein: 8275')\n",
    "print('Number of unique proteins in columns 0 and 1 of the psi file:', len(set(conv_ids.values.reshape(-1))))\n",
    "print('Number of unique UniProt ids:', len(all_unique_uniprot_ids))\n",
    "print('Number of unique ENSP ids:', len(all_unique_ensp_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are interested in the first two columns with uniprot ids\n",
    "# Some of the values in these columns are ensemble ids\n",
    "# lets see if those can be converted to uniprot ids via Biomart\n",
    "# So which proteins are missing\n",
    "compare_to = set(conv_ids.values.reshape(-1))\n",
    "\n",
    "test = all_unique_uniprot_ids.to_list()\n",
    "\n",
    "#get all missing proteins\n",
    "missing_proteins = [x for x in compare_to if x not in test]\n",
    "len(missing_proteins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ENSP00000408321.1',\n",
       " 'ENSP00000470609.1',\n",
       " 'Q9Y2P8-1',\n",
       " 'ENSP00000467185.1',\n",
       " 'ENSP00000411822.1',\n",
       " 'ENSP00000476265.1',\n",
       " 'Q5THR3-2',\n",
       " 'ENSP00000426769.1',\n",
       " 'ENSP00000415200.2',\n",
       " 'ENSP00000493046.1',\n",
       " 'ENSP00000493281.1',\n",
       " 'ENSP00000408168.2',\n",
       " 'ENSP00000324909.5',\n",
       " 'Q96RK0',\n",
       " 'ENSP00000391869.3',\n",
       " 'Q8NA57',\n",
       " 'ENSP00000457957.1',\n",
       " 'ENSP00000482225.1',\n",
       " 'ENSP00000489519.1',\n",
       " 'ENSP00000482011.1',\n",
       " 'Q6NS38-1',\n",
       " 'ENSP00000493302.1',\n",
       " 'Q99569-2',\n",
       " 'Q8IX01-3',\n",
       " 'ENSP00000405973.1',\n",
       " 'ENSP00000473509.2',\n",
       " 'ENSP00000445323.1',\n",
       " 'ENSP00000490118.1',\n",
       " 'ENSP00000465194.1',\n",
       " 'ENSP00000493122.1',\n",
       " 'ENSP00000463483.1']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULT\n",
    "From this point on I accept that we are missing 20 PPIs. The number of gene combinations matches the gene combination tsv file. When I use the same method to get all unique protein ENSP id combination, the number of unique combinations is 20 less than expected.\n",
    "\n",
    "There are now two things we can do:\n",
    "\n",
    "1. Use the UniProt id combinations. This means that we have 51767 PPIs (802 less than the paper claims).<br>\n",
    "2. Use the ENSP id combinations. This means that we have 52549 PPIs (20 less than the paper claims). We are also missing one protein here. It could be that a protein was removed with 20 PPIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_comb_huri_final.to_csv('../Data/Interactome/uniprot_ids_unique_combinations_huri.csv',\n",
    "                               header=False, index=False)\n",
    "ensp_comb_huri_final.to_csv('../Data/Interactome/ensp_ids_unique_combinations_huri.csv',\n",
    "                               header=False, index=False)\n",
    "\n",
    "all_unique_uniprot_ids.to_csv('../Data/Interactome/all_unique_uniprot_ids_huri.csv',\n",
    "                               header=False, index=False)\n",
    "all_unique_ensp_ids.to_csv('../Data/Interactome/all_unique_ensp_ids_huri.csv',\n",
    "                               header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
